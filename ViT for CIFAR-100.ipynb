{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b9cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b26567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def img_to_patch(x, patch_size):\n",
    "    '''\n",
    "    Transforms image into a list of patches of the specified dimensions\n",
    "    '''\n",
    "    B,C,H,W = x.shape\n",
    "    \n",
    "    # Reshape this matrix to B X N X [C * P ** 2]\n",
    "    # If you want to break a dimension into 2, the product of both those\n",
    "    # should equal the original number.\n",
    "    \n",
    "    # Visualized/imagined how the reshaping, permuting and flattening happens. It's \n",
    "    # beautiful to visualize it as well as informative.\n",
    "    x = x.reshape(B,C,H//patch_size,patch_size,W//patch_size,patch_size)\n",
    "    x = x.permute(0,2,4,1,3,5)\n",
    "    x = x.flatten(1,2)\n",
    "    x = x.flatten(2,4)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89a4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my ViT's architecture\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,num_heads,dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.attn = nn.MultiheadAttention(input_dim, num_heads) # Input dim is the number of tokens\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,input_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Apply layer normalization first\n",
    "        out = self.norm1(x)\n",
    "        # Apply multi-headed self-attention\n",
    "        out,_ = self.attn(out,out,out) # Cuz it is self-attention, all the inputs for query,\n",
    "        # key, value come from the same input.\n",
    "        # Apply the residual connection\n",
    "        resid = x + out\n",
    "        # Apply the second layer normalization\n",
    "        out = self.norm2(resid)\n",
    "        # Pass the outputs throught the MLP. Remember that the tokens pass through\n",
    "        # the MLP independently but they share weights ofc.\n",
    "        out = F.gelu(self.fc1(out))\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.drop(out)\n",
    "        \n",
    "        # Apply the second residual connection and give the output.\n",
    "        out = out + resid\n",
    "        return out\n",
    "\n",
    "# Defining the ViTClassifier's architecture\n",
    "class ViTClassifier(nn.Module): \n",
    "    def __init__(self,embed_size,hidden_size,hidden_class_size,num_encoders,num_heads,patch_size,num_patches,dropout = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Important parameters\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        # DNN to create an embedding from flattened patches\n",
    "        self.input = nn.Linear(3*(patch_size**2), embed_size) # 3 cuz it's the no. of channels\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Define the transformer encoders which'll be used for getting the final class token which\n",
    "        # will then be used for classification\n",
    "        self.transformer = nn.Sequential(\n",
    "            *(ViTEncoder(embed_size,hidden_size,num_heads,dropout) for _ in range(num_encoders))\n",
    "        )\n",
    "        \n",
    "        # Defining the classification head and creating the class token and \n",
    "        # learnable position embeddings\n",
    "        self.fc1 = nn.Linear(embed_size,hidden_class_size)\n",
    "        self.fc2 = nn.Linear(hidden_class_size,100) # Since the classification is on the CIFAR-100 DS.\n",
    "        \n",
    "        self.class_embed = nn.Parameter(torch.randn(1,1,embed_size))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1,1 + num_patches,embed_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Flatten patches first from the input and create embeddings\n",
    "        x = img_to_patch(x,self.patch_size)\n",
    "        x = F.relu(self.input(x))\n",
    "        B,N,L = x.shape\n",
    "        \n",
    "        class_embed = self.class_embed.repeat(B,1,1)\n",
    "        x = torch.cat([class_embed, x], dim = 1)\n",
    "        x = x + self.pos_embed[:, :N+1]\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # Applying the transformer encoder\n",
    "        # Transposing because the transformer expects the input in the NXBXembed_size format\n",
    "        # instead of the usual BXNXembed_size format\n",
    "        x = x.transpose(0,1)\n",
    "        x = self.transformer(x)\n",
    "        x = x[0] # An Array of class embeddings. Notice how we concatenated to the beginning\n",
    "        # of the input a couple of steps ago.\n",
    "        \n",
    "        # Classify the class token vector\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4fea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalize with mean and std dev for CIFAR-100\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8791d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/782], Loss: 4.1604\n",
      "Epoch [1/5], Step [200/782], Loss: 4.2520\n",
      "Epoch [1/5], Step [300/782], Loss: 4.4809\n",
      "Epoch [1/5], Step [400/782], Loss: 4.3598\n",
      "Epoch [1/5], Step [500/782], Loss: 4.3857\n",
      "Epoch [1/5], Step [600/782], Loss: 4.4208\n"
     ]
    }
   ],
   "source": [
    "# Defining parameters for training\n",
    "model = ViTClassifier(embed_size=768,hidden_size=512,hidden_class_size=512,num_encoders=4,num_heads=4,patch_size=16,num_patches=16)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print some information every 100 batches\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25df1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
